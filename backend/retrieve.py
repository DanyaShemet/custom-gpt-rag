from dotenv import load_dotenv
import os
import sys
import json
import numpy as np
from openai import OpenAI

# üîß –ü—Ä–∏–º—É—Å–æ–≤–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è stdout —É UTF-8 (–¥–ª—è Windows)
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è .env
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

if not openai_key:
    raise ValueError("OPENAI_API_KEY –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–π —É .env")

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤
if len(sys.argv) < 3:
    print("‚ö†Ô∏è –ù–µ–æ–±—Ö—ñ–¥–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ question —ñ sessionId")
    sys.exit(1)

question = sys.argv[1]
session_id = sys.argv[2]
file_path = f"user_data/user_{session_id}.json"

# ‚õî –Ø–∫—â–æ –±–∞–∑–∞ –∑–Ω–∞–Ω—å –ø–æ—Ä–æ–∂–Ω—è –∞–±–æ –Ω–µ —ñ—Å–Ω—É—î
if not os.path.exists(file_path):
    print("")
    sys.exit(0)

# üß† –ß–∏—Ç–∞–Ω–Ω—è –±–∞–∑–∏ –∑–Ω–∞–Ω—å
with open(file_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# üì¶ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ OpenAI
client = OpenAI(api_key=openai_key)

# üîé –û—Ç—Ä–∏–º–∞–Ω–Ω—è embedding –∑–∞–ø–∏—Ç—É
question_embedding = client.embeddings.create(
    model="text-embedding-3-small",
    input=question
).data[0].embedding

# üîÅ –ö–æ—Å–∏–Ω—É—Å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å
def cosine_similarity(vec1, vec2):
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# üß† –ü–æ—à—É–∫ –Ω–∞–π–±—ñ–ª—å—à —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤
scored_chunks = []
for chunk in chunks:
    chunk_embedding = client.embeddings.create(
        model="text-embedding-3-small",
        input=chunk
    ).data[0].embedding

    score = cosine_similarity(question_embedding, chunk_embedding)
    scored_chunks.append((score, chunk))

scored_chunks.sort(reverse=True, key=lambda x: x[0])
top_chunks = [c[1] for c in scored_chunks[:3]]

# üßæ –í–∏–≤—ñ–¥
print("\n---\n".join(top_chunks))
